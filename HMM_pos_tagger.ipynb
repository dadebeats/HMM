{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dadebeats/HMM/blob/pablo_matrices/HMM_pos_tagger.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Computational syntax\n",
        "\n",
        "**OPTION 1: HMM PoS tagger**\n",
        "\n",
        "**Components of the team:**\n",
        "\n",
        "1. Ibrahim Bukhari\n",
        "\n",
        "2. Adam Kellich\n",
        "\n",
        "3. Nella Zabrina Pramata\n",
        "\n",
        "4. Pablo Tagarro"
      ],
      "metadata": {
        "id": "nkaFnPTWXMUG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- needs to be implemented in github\n",
        "- frequent pushes â€“ it should be clear that the code is updated bit by bit\n",
        "- experiments on 2 datasets from Universal Dependency and compare results\n",
        "- experiments, results, and discussion can be done in Colab\n",
        "- Grade will be based on:\n",
        "1. correct implementation of HMM (30%)\n",
        "2. correctness of experiments (40%)\n",
        "3. analysis of results (30%)\n"
      ],
      "metadata": {
        "id": "u0sB-7IcYB-v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preliminaries\n",
        "\n",
        "*   Importing and Exploring the Dataset\n",
        "*   Splitting the Dataset into Test and Train\n",
        "\n"
      ],
      "metadata": {
        "id": "RIiYsdtER4Aa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "# Mounting Google Drive in Google Colab environment\n",
        "# Mount Google Drive to the '/content/drive' directory\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mfB7mxj-xgx4",
        "outputId": "8a435f4b-2ac2-42ca-b31f-f7a7dfb2b11b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install conllu"
      ],
      "metadata": {
        "id": "RNNbI3_4SE2V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3700257f-6258-4539-fe82-42e86d26c091"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting conllu\n",
            "  Downloading conllu-4.5.3-py2.py3-none-any.whl (16 kB)\n",
            "Installing collected packages: conllu\n",
            "Successfully installed conllu-4.5.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import conllu"
      ],
      "metadata": {
        "id": "xrzL23rwyruJ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_paths = {'English': ['/content/drive/MyDrive/HAP LAP/LAP-CS /English corpus/en_gum-ud-train.conllu', \\\n",
        "                    '/content/drive/MyDrive/HAP LAP/LAP-CS /English corpus/en_gum-ud-dev.conllu', \\\n",
        "                    '/content/drive/MyDrive/HAP LAP/LAP-CS /English corpus/en_gum-ud-test.conllu'], 'Basque':\\\n",
        "                 ['/content/drive/MyDrive/HAP LAP/LAP-CS /Basque corpus/eu_bdt-ud-train.conllu', \\\n",
        "                  '/content/drive/MyDrive/HAP LAP/LAP-CS /Basque corpus/eu_bdt-ud-dev.conllu', \\\n",
        "                  '/content/drive/MyDrive/HAP LAP/LAP-CS /Basque corpus/eu_bdt-ud-test.conllu']}"
      ],
      "metadata": {
        "id": "uL4BQqpjyvKa"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize lists within a dictionary to store parsed data for each dataset\n",
        "parsed_datasets = {\n",
        "    'English': [],\n",
        "    'Basque': []\n",
        "}\n",
        "\n",
        "# Initialize lists within a dictionary to store a tuple of (tokens, tags)\n",
        "tokenized_datasets = {\n",
        "    'English': [],\n",
        "    'Basque': []\n",
        "}\n",
        "\n",
        "# Initialize lists within a dictionary to store a list of tags within each dataset\n",
        "POS_dataset_tags = {\n",
        "    'English': set(),\n",
        "    'Basque': set()\n",
        "}"
      ],
      "metadata": {
        "id": "3-x1Fu113wf0"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Parsing the datasets"
      ],
      "metadata": {
        "id": "B4_93nN32z6Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to read and parse a .conllu file\n",
        "def read_and_parse_conllu(file_path):\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        return conllu.parse(file.read())"
      ],
      "metadata": {
        "id": "9A1PpEreyvEA"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Iterate through each language dataset\n",
        "for language, data_paths in dataset_paths.items():\n",
        "    print(f\"\\nLanguage: {language}\\nData Paths:\\n\\t {data_paths}\\n\")\n",
        "\n",
        "    for data_path in data_paths:\n",
        "        parsed_data = read_and_parse_conllu(data_path)\n",
        "        parsed_datasets[language].append(parsed_data)\n",
        "        print(f\"Parsed data for {language}: {data_path}\")\n",
        "\n",
        "    print(f\"\\nNumber of Sentence Datasets: {len(parsed_datasets[language])}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UZyXVdm02wWA",
        "outputId": "641e7138-1f7a-45ea-ffb0-d84c9fd9e9bf"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Language: English\n",
            "Data Paths:\n",
            "\t ['/content/drive/MyDrive/HAP LAP/LAP-CS /English corpus/en_gum-ud-train.conllu', '/content/drive/MyDrive/HAP LAP/LAP-CS /English corpus/en_gum-ud-dev.conllu', '/content/drive/MyDrive/HAP LAP/LAP-CS /English corpus/en_gum-ud-test.conllu']\n",
            "\n",
            "Parsed data for English: /content/drive/MyDrive/HAP LAP/LAP-CS /English corpus/en_gum-ud-train.conllu\n",
            "Parsed data for English: /content/drive/MyDrive/HAP LAP/LAP-CS /English corpus/en_gum-ud-dev.conllu\n",
            "Parsed data for English: /content/drive/MyDrive/HAP LAP/LAP-CS /English corpus/en_gum-ud-test.conllu\n",
            "\n",
            "Number of Sentence Datasets: 3\n",
            "\n",
            "\n",
            "Language: Basque\n",
            "Data Paths:\n",
            "\t ['/content/drive/MyDrive/HAP LAP/LAP-CS /Basque corpus/eu_bdt-ud-train.conllu', '/content/drive/MyDrive/HAP LAP/LAP-CS /Basque corpus/eu_bdt-ud-dev.conllu', '/content/drive/MyDrive/HAP LAP/LAP-CS /Basque corpus/eu_bdt-ud-test.conllu']\n",
            "\n",
            "Parsed data for Basque: /content/drive/MyDrive/HAP LAP/LAP-CS /Basque corpus/eu_bdt-ud-train.conllu\n",
            "Parsed data for Basque: /content/drive/MyDrive/HAP LAP/LAP-CS /Basque corpus/eu_bdt-ud-dev.conllu\n",
            "Parsed data for Basque: /content/drive/MyDrive/HAP LAP/LAP-CS /Basque corpus/eu_bdt-ud-test.conllu\n",
            "\n",
            "Number of Sentence Datasets: 3\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Implementation of HMM"
      ],
      "metadata": {
        "id": "6BkARkuJYbcc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Part 1:** Calculating the Emission and Transition Matrix"
      ],
      "metadata": {
        "id": "MIueTG_SQM-S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1: Calculate the Emission Matrix of the Dataset"
      ],
      "metadata": {
        "id": "-Jl3pacmPjD8"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wqbnonqQQjL0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2: Calculate the Transition Matrix of the Dataset"
      ],
      "metadata": {
        "id": "JogJ4OQLP3_0"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tMvR00soQidN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Part 2:** Implementing the Veterbi Algorithim  "
      ],
      "metadata": {
        "id": "c4E7UhMcQpXy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.3 Use the Veterbi algorithim to find the best path\n"
      ],
      "metadata": {
        "id": "vdpWdrqdP5Ta"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NpCHEN1SXHyx"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Experiments"
      ],
      "metadata": {
        "id": "vE1B4pQrYlXy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Use accuracy\n",
        "- compare between different types of HMM e.g comparing between smoothing etc"
      ],
      "metadata": {
        "id": "Plc-faTZFCg4"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "miMsRaPFYu_Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Analysis of results"
      ],
      "metadata": {
        "id": "PXlGwLV7Yv0I"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "beTrUXHBY1Pg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "b-_O1D_nY0n5"
      }
    }
  ]
}