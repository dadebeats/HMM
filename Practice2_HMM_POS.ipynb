{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **POS** tagger by using **Hidden Markov Models**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imp Note: *Dev dataset is used to verify the unknown tags in both emission and transition matrices. We will also have an additional categoruy in the training dataset which wil have the UNK tag that contains unkown. The dev dataset will also be used for smoothing and more. We will categorize the most infrequent words into UNK.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing all the libraries here\n",
    "import os\n",
    "import conllu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Converting Data into Tokens and Tags "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing The Global Variables\n",
    "\n",
    "- \"dataset_paths\" is a dictionary which has the language's names as its keys. It consists of a list that contains the file location of the traning, dev and testing datasets\n",
    "- \"parsed_datasets\" is a dictionary which has the language's names as its keys. It consists if a list with all the datasets which have been parsed into sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- \"tokenized_datasets\" is a dictionary which has a list consisting of three further lists for training, dev and testing. \n",
    "    - tokenized_datasets[ language ][ 0 ] = training dataset list of tuples, (tokens, tags)\n",
    "    - tokenized_datasets[ language ][ 1 ] = dev dataset list of tuples, (tokens, tags)\n",
    "    - tokenized_datasets[ language ][ 2 ] = testing dataset list of tuples, (tokens, tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- \"POS_dataset_tags\" is a dictionary which stores all the tags within a language "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the paths to the .conllu datasets\n",
    "dataset_paths = {\n",
    "    'English': ['Data/English/en_gum-ud-train.conllu', 'Data/English/en_gum-ud-dev.conllu', 'Data/English/en_gum-ud-test.conllu'],\n",
    "    'Basque':  ['Data/Basque/eu_bdt-ud-train.conllu', 'Data/Basque/eu_bdt-ud-dev.conllu', 'Data/Basque/eu_bdt-ud-test.conllu']\n",
    "}\n",
    "\n",
    "# Initialize lists within a dictionary to store parsed data for each dataset\n",
    "parsed_datasets = {\n",
    "    'English': [],\n",
    "    'Basque': []\n",
    "}\n",
    "\n",
    "# Initialize lists within a dictionary to store a tuple of (tokens, tags)\n",
    "tokenized_datasets = {\n",
    "    'English': [],\n",
    "    'Basque': []\n",
    "}\n",
    "\n",
    "# Initialize lists within a dictionary to store a list of tags within each dataset\n",
    "POS_dataset_tags = {\n",
    "    'English': set(),\n",
    "    'Basque': set()\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing the Data Files\n",
    "\n",
    "- In the following code we will parse the .conllu files and store all the three \"Training, Dev and Testing\" datasets of both languages into the \"Paresed_datasets\" variable.\n",
    "- We will create a function \"read_and_parse_conllu\" which takes \"file_path\" as a parameter and returns conllu parsed class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to read and parse a .conllu file\n",
    "def read_and_parse_conllu(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        return conllu.parse(file.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Language: English\n",
      "Data Paths:\n",
      "\t ['Data/English/en_gum-ud-train.conllu', 'Data/English/en_gum-ud-dev.conllu', 'Data/English/en_gum-ud-test.conllu']\n",
      "\n",
      "Parsed data for English: Data/English/en_gum-ud-train.conllu\n",
      "Parsed data for English: Data/English/en_gum-ud-dev.conllu\n",
      "Parsed data for English: Data/English/en_gum-ud-test.conllu\n",
      "\n",
      "Number of Sentence Datasets: 3\n",
      "\n",
      "\n",
      "Language: Basque\n",
      "Data Paths:\n",
      "\t ['Data/Basque/eu_bdt-ud-train.conllu', 'Data/Basque/eu_bdt-ud-dev.conllu', 'Data/Basque/eu_bdt-ud-test.conllu']\n",
      "\n",
      "Parsed data for Basque: Data/Basque/eu_bdt-ud-train.conllu\n",
      "Parsed data for Basque: Data/Basque/eu_bdt-ud-dev.conllu\n",
      "Parsed data for Basque: Data/Basque/eu_bdt-ud-test.conllu\n",
      "\n",
      "Number of Sentence Datasets: 3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Iterate through each language dataset\n",
    "for language, data_paths in dataset_paths.items():\n",
    "    print(f\"\\nLanguage: {language}\\nData Paths:\\n\\t {data_paths}\\n\")\n",
    "    \n",
    "    for data_path in data_paths:\n",
    "        parsed_data = read_and_parse_conllu(data_path)\n",
    "        parsed_datasets[language].append(parsed_data)\n",
    "        print(f\"Parsed data for {language}: {data_path}\")\n",
    "    \n",
    "    print(f\"\\nNumber of Sentence Datasets: {len(parsed_datasets[language])}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the Parsed Datasets\n",
    "\n",
    "Here we explore the basic information regrading the datasets which we have just parsed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset of Language 1: English\n",
      "Number of Sentence Datasets: 3\n",
      "English Sentences in the training Dataset: 8548\n",
      "English Sentences in the dev Dataset: 1117\n",
      "English Sentences in the testing Dataset: 1096\n",
      "\n",
      "Dataset of Language 2: Basque\n",
      "Number of Sentence Datasets: 3\n",
      "Basque Sentences in the training Dataset: 5396\n",
      "Basque Sentences in the dev Dataset: 1798\n",
      "Basque Sentences in the testing Dataset: 1799\n"
     ]
    }
   ],
   "source": [
    "# Iterate through the languages and their corresponding datasets\n",
    "for idx, language in enumerate(parsed_datasets.keys()):\n",
    "    print(f\"\\nDataset of Language {idx + 1}: {language}\")\n",
    "\n",
    "    # Define dataset names\n",
    "    dataset_names = [\"training\", \"dev\", \"testing\"]\n",
    "\n",
    "    # Print the number of sentence datasets for each language\n",
    "    print(f\"Number of Sentence Datasets: {len(parsed_datasets[language])}\")\n",
    "\n",
    "    # Iterate through the sentence datasets for each language\n",
    "    for idx, sentence_dataset in enumerate(parsed_datasets[language]):\n",
    "        dataset_name = dataset_names[idx]\n",
    "\n",
    "        # Print the number of sentences in each dataset for the current language\n",
    "        print(f\"{language} Sentences in the {dataset_name} Dataset: {len(sentence_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsed Dataset into Token and Tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we Extract the words(form) and POS(upos - universal part of speech) and store them within a tuple that is appended to our list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for language in parsed_datasets.keys():\n",
    "    # English, Basque\n",
    "    \n",
    "    for sentence_dataset in parsed_datasets[language]:\n",
    "        # 0 train, 1 dev, 2 test\n",
    "        temp_tokenized_list = []\n",
    "        \n",
    "        for sentence in sentence_dataset:\n",
    "            # Single Sentence form the one of the datasets from above\n",
    "            \n",
    "            for token in sentence:\n",
    "                # single token from the sentence\n",
    "\n",
    "                temp_tokenized_list.append((token['form'], token['upos']))\n",
    "                POS_dataset_tags[language].add(token['upos'])\n",
    "        \n",
    "        tokenized_datasets[language].append(temp_tokenized_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the Tokenized and Tagged Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we explore the token size and all the Tags which are within a certain language. Furthemore we also see some examples of each dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English Train Tokens: 150143\n",
      "Example: [('Aesthetic', 'ADJ'), ('Appreciation', 'NOUN'), ('and', 'CCONJ'), ('Spanish', 'ADJ'), ('Art', 'NOUN')]\n",
      "English Dev Tokens: 19964\n",
      "Example: [('Introduction', 'NOUN'), ('Research', 'NOUN'), ('on', 'ADP'), ('adult', 'NOUN'), ('-', 'PUNCT')]\n",
      "English Test Tokens: 20171\n",
      "Example: [('The', 'DET'), ('prevalence', 'NOUN'), ('of', 'ADP'), ('discrimination', 'NOUN'), ('across', 'ADP')]\n",
      "\n",
      "Basque Train Tokens: 72974\n",
      "Example: [('Gero', 'ADV'), (',', 'PUNCT'), ('lortutako', 'VERB'), ('masa', 'NOUN'), ('molde', 'NOUN')]\n",
      "Basque Dev Tokens: 24095\n",
      "Example: [('Atenasen', 'PROPN'), ('ordea', 'CCONJ'), (',', 'PUNCT'), ('beste', 'DET'), ('bost', 'NUM')]\n",
      "Basque Test Tokens: 24374\n",
      "Example: [('Familian', 'NOUN'), (',', 'PUNCT'), ('aldiz', 'CCONJ'), (',', 'PUNCT'), ('ez', 'PART')]\n",
      "\n",
      "English POS Tags list: {'PROPN', 'PRON', 'NOUN', '_', 'NUM', 'PUNCT', 'VERB', 'PART', 'ADP', 'AUX', 'ADV', 'DET', 'X', 'SCONJ', 'ADJ', 'INTJ', 'SYM', 'CCONJ'}\n",
      "Basque POS Tags list: {'PROPN', 'PRON', 'NOUN', 'NUM', 'PUNCT', 'VERB', 'PART', 'ADP', 'AUX', 'ADV', 'DET', 'X', 'SCONJ', 'ADJ', 'INTJ', 'SYM', 'CCONJ'}\n"
     ]
    }
   ],
   "source": [
    "languages = ['English', 'Basque']\n",
    "\n",
    "# Iterating through each language\n",
    "for language in languages:\n",
    "    for dataset_type, tokens in zip(['Train', 'Dev', 'Test'], tokenized_datasets[language]):\n",
    "        print(f\"{language} {dataset_type} Tokens: {len(tokens)}\")\n",
    "        print(f\"Example: {tokens[:5]}\")\n",
    "    print() # for better readability, no other purpose\n",
    "\n",
    "# Printing the Part of Speech of both Languages\n",
    "for language in languages:\n",
    "    print(f\"{language} POS Tags list: {POS_dataset_tags[language]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Implementing HMM and Viterbi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will implement the Hidden Markov model by calculating the Emission matrix as well as the Transition matrix of the Tokens. Then after that we will use the Viterbi algorithm to calculate the best path of a given input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Calculating the Emission Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.3 ('venvSyntax': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8039dc67c7fc3bb81eba0b7706ae90869074cd71063e3b75dec25ad99fd8f24e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
